{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1kjL8wUTvAe"
   },
   "source": [
    "# **Multi-Objective Variational Autoencoder with 3D Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxpFKwDcTpqC"
   },
   "source": [
    "## Prepare necessary libraries and build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93OZswxKM-F-",
    "outputId": "47ba0d46-8851-4e8c-9144-b10e3cf502a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================== IMPORT LIBRARIES ==================\n",
    "import os\n",
    "\n",
    "# This allow a silent import of Tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "from statistics import mean\n",
    "from google_drive_downloader import GoogleDriveDownloader\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "print(\"Successfully Load All Libraries - Tensorflow Version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LLrQsldk6dy",
    "outputId": "0fb77cbd-5fc6-49f2-8acc-24fe2c5168f6"
   },
   "outputs": [],
   "source": [
    "# ================= SET UP PARAMETERS ==================\n",
    "\n",
    "# The latent dimension of the variational autoencoder network\n",
    "latent_dimension = 10\n",
    "\n",
    "# During the training, a number of samples using random latent values will be generated. This defines how many number of samples we want to generate\n",
    "num_examples_to_generate = 25\n",
    "\n",
    "# Epochs\n",
    "epochs = 50\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Path of the directory where all the Json files are saved. \n",
    "data_files_directory = './data/chair_voxel_dataset/'\n",
    "\n",
    "\n",
    "# Path of the directory where the pretrained weights of the vae will be loaded from. \n",
    "vae_load_location = './data/pretrained_models/chair_vae/epoch_100'\n",
    "\n",
    "# Path of the directory where the pretrained weights of the stability auxiliary discriminator will be loaded from. \n",
    "stability_auxiliary_discriminator_load_location = './data/pretrained_models/chair_stability_auxiliary_discriminator/epoch_50'\n",
    "\n",
    "# Path of the directory where the pretrained weights of the aesthetic auxiliary discriminator will be loaded from. \n",
    "aesthetic_auxiliary_discriminator_load_location = './data/pretrained_models/chair_aesthetic_auxiliary_discriminator/epoch_50'\n",
    "\n",
    "# Path of the directory where the pretrained weights of the function auxiliary discriminator will be loaded from. \n",
    "function_auxiliary_discriminator_load_location = './data/pretrained_models/chair_function_auxiliary_discriminator/epoch_50'\n",
    "\n",
    "# Path of the directory where the trained weights of the network will be saved. \n",
    "mo_vae_save_directory = './movae_train/saved_models/'\n",
    "\n",
    "# Create the directory to save the trained weights of the network. \n",
    "try:\n",
    "    os.makedirs(mo_vae_save_directory)\n",
    "except FileExistsError:\n",
    "    print(\"The directory to save trained network weights already exists\")\n",
    "\n",
    "# The path of log file that contains training information. \n",
    "mo_vae_training_log_location = './movae_train/logs.txt'\n",
    "\n",
    "# Create a txt file to save losses during training\n",
    "try:\n",
    "    log_file = open(mo_vae_training_log_location, \"x\")\n",
    "    log_file.close()\n",
    "except FileExistsError:\n",
    "    print(\"Log file already exists\")\n",
    "    \n",
    "# During the training, a number of samples using random latent values will be generated. This defines where these generated samples will be saved\n",
    "directory_to_generated_samples = './movae_train/samples/'\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory_to_generated_samples)\n",
    "except FileExistsError:\n",
    "    print(\"The directory to save generated samples already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrjQdCcZXMLl"
   },
   "source": [
    "Download the dataset. \n",
    "Each file is a JSON file that contains information of the geometry we converted from the ShapeNet dataset and the related ratings. \n",
    "\n",
    "The JSON file should have following properties: \n",
    "\n",
    "*   \"geometry\":  a multi-dimension array to represent the geometry. \n",
    "*   \"voxel_size\": the size of each voxels (should always equal to 1 in the context of this notebook)\n",
    "*   \"scaling_factor_from_original\": the scaling factor we used to scale the original mesh to produce this geometry\n",
    "*   \"geometry_shape\": a list representing the shape of the geometry. (should always be [32, 32, 64] in the context of this notebook)\n",
    "\n",
    "Ratings in the JSON files are as following: \n",
    "\n",
    "If the geometry is not given one of the ratings, the properties will note null\n",
    "*   \"stability_rating\": the stability rating as a float from 0 to 1\n",
    "*   \"aesthetic_rating\": the aesthetic rating as an integer from 0 to 10\n",
    "*   \"function_rating\": the function rating as an integer from 0 to 10\n",
    "\n",
    "Although the aesthetic_rating and function_rating are from 0 to 10 in the dataset. They should be normalized for the training, including the pretrained network should also produce a normalized rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If the directory exists and is not empty we presume the dataset is already on the local drive. \n",
    "# If not, we will download the dataset. \n",
    "\n",
    "download_dataset_from_google_drive = False\n",
    "\n",
    "if os.path.isdir(data_files_directory):\n",
    "    if not os.listdir(data_files_directory):\n",
    "        # Directory is empty\n",
    "        download_dataset_from_google_drive = True\n",
    "    else:    \n",
    "        # Directory is not empty\n",
    "        download_dataset_from_google_drive = False\n",
    "else:\n",
    "    # Given directory doesn't exist\n",
    "    download_dataset_from_google_drive = True\n",
    "    \n",
    "if download_dataset_from_google_drive:\n",
    "\n",
    "    # This will get download all the json files that contain geometry and rating information from google drive to the destination directory\n",
    "    GoogleDriveDownloader.download_file_from_google_drive(file_id='1DJzSfSWUDkAhuEfu706pm0riC6ho_5NS', \n",
    "                                                          dest_path=os.path.join(data_files_directory, 'chair_voxel_dataset.zip'),\n",
    "                                                          unzip=True)\n",
    "    data_files_directory = data_files_directory + 'chair_voxel_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the json file paths from the unzippped directory. \n",
    "data_file_paths = glob.glob(data_files_directory + \"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_with_stability_rating = []\n",
    "data_files_with_aesthetic_rating = []\n",
    "data_files_with_function_rating = []\n",
    "\n",
    "for data_file in data_file_paths:\n",
    "    with open(data_file) as json_file:\n",
    "        dictionary = json.load(json_file)\n",
    "        if dictionary[\"stability_rating\"] != None:\n",
    "            data_files_with_stability_rating.append(data_file)\n",
    "        if dictionary[\"aesthetic_rating\"] != None:\n",
    "            data_files_with_aesthetic_rating.append(data_file)\n",
    "        if dictionary[\"function_rating\"] != None:\n",
    "            data_files_with_function_rating.append(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} json files in total. \".format(len(data_file_paths)))\n",
    "print(\"There are {} files that have stability rating. \".format(len(data_files_with_stability_rating)))\n",
    "print(\"There are {} files that have aesthetic rating. \".format(len(data_files_with_aesthetic_rating)))\n",
    "print(\"There are {} files that have function rating. \".format(len(data_files_with_function_rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many samples will be used for training\n",
    "num_training_samples = 4750  # 4750\n",
    "\n",
    "# Define how many samples will be used for validation\n",
    "num_validation_samples = 2027  # 2027"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several helper functions below that will facilitate building the tensorflow dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= HELPER FUNCTIONS ===================\n",
    "# This function can preview a geometry data using matplot lib\n",
    "def preview_geometries(geometry_data, plot_threshold=0.25, stability_model=None, aesthetic_model=None, function_model=None, save_plot_as_png=False, path_to_save=None):\n",
    "    # geometry_data must be an array of shape [32, 32, 64, 1]\n",
    "    if stability_model is not None:\n",
    "        stability_prediction = np.array(stability_model(np.expand_dims(geometry_data, axis=0)))[0][0]\n",
    "    else:\n",
    "        stability_prediction = None\n",
    "    \n",
    "    if aesthetic_model is not None:\n",
    "        aesthetic_prediction = np.array(aesthetic_model(np.expand_dims(geometry_data, axis=0)))[0][0]\n",
    "    else:\n",
    "        aesthetic_prediction = None\n",
    "    \n",
    "    if function_model is not None:\n",
    "        function_prediction = np.array(function_model(np.expand_dims(geometry_data, axis=0)))[0][0]\n",
    "    else:\n",
    "        function_prediction = None\n",
    "    \n",
    "    generation_title = \"Stability Prediction is {}\\nAesthetic Prediction is {}\\nFunction Prediction is {}\".format(str(stability_prediction), str(aesthetic_prediction), str(function_prediction))\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    data = np.squeeze(geometry_data, axis=-1)\n",
    "    for i in range(0, len(data)):\n",
    "        for j in range(0, len(data[i])):\n",
    "            for k in range(0, len(data[i][j])):\n",
    "                if data[i][j][k] >= plot_threshold:\n",
    "                    x.append(i)\n",
    "                    y.append(j)\n",
    "                    z.append(k)\n",
    "\n",
    "    z_c = z\n",
    "\n",
    "    # We decided to plot the results with a gradient color map so the depth and geometries are clearer to see. \n",
    "    from matplotlib.colors import ListedColormap\n",
    "\n",
    "    N = 256\n",
    "    vals = np.ones((N, 4))\n",
    "    vals[:, 0] = np.linspace(1, 0, N)\n",
    "    vals[:, 1] = np.linspace(43/N, 43/N, N)\n",
    "    vals[:, 2] = np.linspace(82/N, 82/N, N)\n",
    "    custom_cmp = ListedColormap(vals)\n",
    "\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    fig = plt.axes(projection='3d')\n",
    "    \n",
    "    # Data for three-dimensional scattered points\n",
    "    fig.scatter3D(np.array(x), np.array(y) * -1, np.array(z), cmap=custom_cmp, c=z_c)\n",
    "    fig.set_zlim(0,64)\n",
    "    fig.set_ylim(-64,0)\n",
    "    fig.set_xlim(0,64)\n",
    "    plt.title(generation_title);\n",
    "    if (save_plot_as_png):\n",
    "        plt.savefig(path_to_save + '.png')\n",
    "  \n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "# This function reads a custom dictionary data\n",
    "def read_json_file_geometry(filepath):\n",
    "    with open(filepath.numpy()) as json_file:\n",
    "        dictionary = json.load(json_file)\n",
    "\n",
    "    tensor = tf.convert_to_tensor(np.array(dictionary[\"geometry\"]), dtype=tf.float32)\n",
    "    tensor = tf.expand_dims(tensor, -1)\n",
    "    return tensor\n",
    "\n",
    "# Function to build dataset without labels\n",
    "def build_dataset_wo_labels(filepaths, batch_size):\n",
    "    file_list = tf.data.Dataset.list_files(filepaths)\n",
    "    ds = file_list.map(lambda x: tf.py_function(read_json_file_geometry, [x], Tout=tf.float32))\n",
    "    ds = ds.shuffle(4, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(4)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tensorflow dataset\n",
    "\n",
    "train_ds = build_dataset_wo_labels(data_file_paths[0:num_training_samples], batch_size)\n",
    "val_ds = build_dataset_wo_labels(data_file_paths[num_training_samples:num_training_samples + num_validation_samples], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can preview the first geometry in first batch as following\n",
    "\n",
    "for each_batch in train_ds:\n",
    "    preview_geometries(each_batch[0], plot_threshold=0.25, stability_model=None, aesthetic_model=None, function_model=None, save_plot_as_png=False, path_to_save=None)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained networks for multi-objective variational autoencoder training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the variational autoencoder network and load the pretrained weights. \n",
    "\n",
    "The weights to load must match structure as the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xk5sOe1kLgo"
   },
   "outputs": [],
   "source": [
    "# ================ SET UP MACHINE LEARNING MODELS =========\n",
    "class Conv3DVariationalAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Conv3DVariationalAutoencoder, self).__init__()  #\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=[32, 32, 64, 1]),\n",
    "\n",
    "                tf.keras.layers.Conv3D(filters=32, kernel_size=(5, 5, 5), strides=(1, 1, 1), padding=\"same\", activation='relu'),\n",
    "                # 32*32*64*32\n",
    "                tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2)),\n",
    "                # 16x16x32*32\n",
    "\n",
    "                tf.keras.layers.Conv3D(filters=64, kernel_size=(5, 5, 5), strides=(1, 1, 1), padding=\"same\", activation='relu'),\n",
    "                # 16x16x32*64\n",
    "                tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2)),\n",
    "                # 8x8x16*64\n",
    "\n",
    "                tf.keras.layers.Conv3D(filters=128, kernel_size=(5, 5, 5), strides=(1, 1, 1), padding=\"same\", activation='relu'),\n",
    "                # 8x8x16*128\n",
    "                tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2)),\n",
    "                # 4x4x8*128\n",
    "\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "\n",
    "                tf.keras.layers.Dense(units=4*4*8*128, activation='relu'),\n",
    "                tf.keras.layers.Reshape(target_shape=(4, 4, 8, 128)),\n",
    "\n",
    "                tf.keras.layers.Conv3DTranspose(filters=128, kernel_size=(5, 5, 5), strides=(2, 2, 2), padding=\"same\", activation='relu'),\n",
    "                tf.keras.layers.Conv3DTranspose(filters=64, kernel_size=(5, 5, 5), strides=(2, 2, 2), padding=\"same\", activation='relu'),\n",
    "                tf.keras.layers.Conv3DTranspose(filters=32, kernel_size=(5, 5, 5), strides=(2, 2, 2), padding=\"same\", activation='relu'),\n",
    "                tf.keras.layers.Conv3DTranspose(filters=1, kernel_size=(5, 5, 5), padding=\"same\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def call(self, inp):\n",
    "        return self.sample_from_input(inp)\n",
    "\n",
    "    def sample_from_latent(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=False)\n",
    "\n",
    "    def sample_from_input(self, input_data):\n",
    "        mean, logvar = self.encode(input_data)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstruction = self.decode(z, apply_sigmoid=True)\n",
    "        return reconstruction\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d_vae = Conv3DVariationalAutoencoder(latent_dimension)\n",
    "\n",
    "conv3d_vae.build((batch_size, 32, 32, 64, 1))\n",
    "conv3d_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efZdWp61iKDl",
    "outputId": "eefd6a13-2ca6-4006-f1c2-101c4f87c5e5"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    conv3d_vae.load_weights(vae_load_location)\n",
    "    \n",
    "# If the pretrained weights cannot be loaded from the directory, we download the weights from google drive trained by researchers. \n",
    "except:\n",
    "    GoogleDriveDownloader.download_file_from_google_drive(file_id='1iGAA9RSQ-_WFV1mE37SZUHj2K8WpT24r', \n",
    "                                                          dest_path='./data/pretrained_models/chair_vae.zip',\n",
    "                                                          unzip=True)\n",
    "    conv3d_vae.load_weights('./data/pretrained_models/chair_vae/epoch_100')\n",
    "\n",
    "print(\"Successfully loaded vae weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the stability auxiliary discriminator network and load the pretrained weights.\n",
    "\n",
    "The weights to load must match structure as the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_auxiliary_discriminator_model = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.Conv3D(16, (4, 4, 4), strides=(2, 2, 2), padding='same', input_shape=(32, 32, 64, 1)), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Conv3D(32, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Conv3D(64, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Conv3D(128, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1), \n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stability_auxiliary_discriminator_model.load_weights(stability_auxiliary_discriminator_load_location)\n",
    "    \n",
    "# If the pretrained weights cannot be loaded from the directory, we download the weights from google drive trained by researchers. \n",
    "except:\n",
    "    GoogleDriveDownloader.download_file_from_google_drive(file_id='1bDwYinD8Tshf0GqnV-ktgRS8hDCUb1oZ', \n",
    "                                                          dest_path='./data/pretrained_models/chair_stability_auxiliary_discriminator.zip',\n",
    "                                                          unzip=True)\n",
    "    stability_auxiliary_discriminator_model.load_weights('./data/pretrained_models/chair_stability_auxiliary_discriminator/epoch_50')\n",
    "\n",
    "print(\"Successfully loaded stability auxiliary discriminator weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the aesthetic auxiliary discriminator network and load the pretrained weights.\n",
    "\n",
    "The weights to load must match structure as the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aesthetic_auxiliary_discriminator_model = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.Conv3D(16, (4, 4, 4), strides=(2, 2, 2), padding='same', input_shape=(32, 32, 64, 1)), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Conv3D(32, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Conv3D(64, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Conv3D(128, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1), \n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    aesthetic_auxiliary_discriminator_model.load_weights(aesthetic_auxiliary_discriminator_load_location)\n",
    "    \n",
    "# If the pretrained weights cannot be loaded from the directory, we download the weights from google drive trained by researchers. \n",
    "except:\n",
    "    GoogleDriveDownloader.download_file_from_google_drive(file_id='1clRiK4mjHAi8jwR0F84HzEebtkhrryfA', \n",
    "                                                          dest_path='./data/pretrained_models/chair_aesthetic_auxiliary_discriminator.zip',\n",
    "                                                          unzip=True)\n",
    "    aesthetic_auxiliary_discriminator_model.load_weights('./data/pretrained_models/chair_aesthetic_auxiliary_discriminator/epoch_50')\n",
    "    \n",
    "print(\"Successfully loaded aesthetic auxiliary discriminator weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the function auxiliary discriminator network and load the pretrained weights.\n",
    "\n",
    "The weights to load must match structure as the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_auxiliary_discriminator_model = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.Conv3D(16, (4, 4, 4), strides=(2, 2, 2), padding='same', input_shape=(32, 32, 64, 1)), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "   \n",
    "    tf.keras.layers.Conv3D(32, (4, 4, 4), strides=(2, 2, 2), padding='same'), \n",
    "    tf.keras.layers.LeakyReLU(), \n",
    "    tf.keras.layers.Dropout(0.3), \n",
    "   \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1), \n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    function_auxiliary_discriminator_model.load_weights(function_auxiliary_discriminator_load_location)\n",
    "    \n",
    "# If the pretrained weights cannot be loaded from the directory, we download the weights from google drive trained by researchers. \n",
    "except:\n",
    "    GoogleDriveDownloader.download_file_from_google_drive(file_id='1v2r9nfhA6HqHFZZ0xZ2W7u5X2pNF2eLp', \n",
    "                                                          dest_path='./data/pretrained_models/chair_function_auxiliary_discriminator.zip',\n",
    "                                                          unzip=True)\n",
    "    function_auxiliary_discriminator_model.load_weights('./data/pretrained_models/chair_function_auxiliary_discriminator/epoch_50')\n",
    "    \n",
    "print(\"Successfully loaded function auxiliary discriminator weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the multi-objective variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the expectations for each category (the values are normalized)\n",
    "\n",
    "Define the LAMBDA value (the weight) of each category. Because these weights are relative to the reconstruction score of the encoder, the researchers experimented with these numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_expectation = 0.05\n",
    "aesthetic_expectation = 0.05\n",
    "function_expectation = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_obj = [50000, 60000, 60000]  # [stability, aesthetic, function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open(mo_vae_training_log_location, \"w\")\n",
    "log_file.write(\"Expectation Values for [stability, aesthetic, function]: {}\".format([stability_expectation, aesthetic_expectation, function_expectation]))\n",
    "log_file.write('\\n')\n",
    "log_file.write(\"Lambda Values for [stability, aesthetic, function]: {}\".format(LAMBDA_obj))\n",
    "log_file.write('\\n')\n",
    "log_file.write(\"Learning Rate for this training: {}\".format(learning_rate))\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n",
    "\n",
    "\n",
    "def compute_loss(model, original, label):\n",
    "    mean, logvar = model.encode(original)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=label)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "\n",
    "def compute_mo_vae_loss(generation_loss, obj_discriminator_pred, LAMBDA_obj, obj_discriminator_exp):\n",
    "    # vae_loss = compute_loss(model, original, label)\n",
    "    \n",
    "    # objective discriminator loss using mean absolute error\n",
    "    discriminator_obj_loss = tf.reduce_mean(tf.abs(obj_discriminator_pred - obj_discriminator_exp), axis=0)\n",
    "\n",
    "    if len(LAMBDA_obj) == 1:\n",
    "        weighted_discriminator_obj_loss = tf.math.reduce_sum(LAMBDA_obj[0] * discriminator_obj_loss)\n",
    "    else:\n",
    "        weighted_disc_obj_losses_as_tensor = tf.math.multiply(tf.cast(LAMBDA_obj, tf.float32), discriminator_obj_loss)\n",
    "        obj_1_loss = tf.math.reduce_mean(weighted_disc_obj_losses_as_tensor[0])\n",
    "        obj_2_loss = tf.math.reduce_mean(weighted_disc_obj_losses_as_tensor[1])\n",
    "        obj_3_loss = tf.math.reduce_mean(weighted_disc_obj_losses_as_tensor[2])\n",
    "        weighted_discriminator_obj_loss = tf.math.reduce_sum(tf.math.multiply(tf.cast(LAMBDA_obj, tf.float32), discriminator_obj_loss))\n",
    "\n",
    "    total_generator_loss = generation_loss + weighted_discriminator_obj_loss\n",
    "\n",
    "    return total_generator_loss, obj_1_loss, obj_2_loss, obj_3_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def mo_vae_train_step(opt, original, label, train=False):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        generator = conv3d_vae.layers[1]\n",
    "\n",
    "        tape.watch(generator.trainable_variables)\n",
    "\n",
    "        generated_geometry = conv3d_vae.sample_from_input(original)\n",
    "\n",
    "        objective_stability = stability_auxiliary_discriminator_model(generated_geometry)\n",
    "        expectation_stability = tf.ones_like(objective_stability) * stability_expectation\n",
    "        objective_aesthetic = aesthetic_auxiliary_discriminator_model(generated_geometry)\n",
    "        expectation_aesthetic = tf.ones_like(objective_aesthetic) * aesthetic_expectation\n",
    "        objective_function = function_auxiliary_discriminator_model(generated_geometry)\n",
    "        expectation_function = tf.ones_like(objective_function) * function_expectation\n",
    "\n",
    "        objective_list = tf.concat([objective_stability, objective_aesthetic, objective_function], axis=-1)\n",
    "        expectation_list = tf.concat([expectation_stability, expectation_aesthetic, expectation_function], axis=-1)\n",
    "    \n",
    "        # ATTENTION: No VAE loss calculated\n",
    "        loss_val, loss_obj_stability, loss_obj_aesthetic, loss_obj_function = compute_mo_vae_loss(0, objective_list, LAMBDA_obj, expectation_list)\n",
    "    \n",
    "        # ATTENTION: With VAE loss calculated\n",
    "        #loss_val, loss_obj_appearance, loss_obj_otfactor, loss_obj_leisurework = compute_mo_vae_loss(compute_loss(conv3d_vae, original, label), objective_list, LAMBDA_obj, expectation_list)\n",
    "    \n",
    "        # loss_val = compute_loss(vae_model, original, label)\n",
    "        gradients = tape.gradient(loss_val, generator.trainable_variables)\n",
    "        gradient_variables = zip(gradients, generator.trainable_variables)\n",
    "        if train:\n",
    "            opt.apply_gradients(gradient_variables)\n",
    "    \n",
    "    return loss_val, loss_obj_stability, loss_obj_aesthetic, loss_obj_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sample_json(model, noise_input, directory_to_write, filename_to_write):\n",
    "    predictions = model.decode(noise_input, apply_sigmoid=True)\n",
    "    for i in range(predictions.shape[0]): # each num_examples_to_generate\n",
    "        # Create the information to write\n",
    "        dictionary_to_write = {\n",
    "            \"geometry\" : np.array(predictions[i], dtype=np.float64).tolist(),\n",
    "            \"voxel_size\" : 1,\n",
    "            \"scaling_factor_from_original\" : None,\n",
    "            \"geometry_shape\" : [32, 32, 64],\n",
    "\n",
    "            # Could also include the discriminator predictions\n",
    "            \"stability_discriminator_prediction\": np.array(stability_auxiliary_discriminator_model(tf.expand_dims(predictions[i], axis=0)), dtype=np.float64)[0][0],\n",
    "            \"aesthetic_discriminator_prediction\": np.array(aesthetic_auxiliary_discriminator_model(tf.expand_dims(predictions[i], axis=0)), dtype=np.float64)[0][0], \n",
    "            \"function_discriminator_prediction\": np.array(function_auxiliary_discriminator_model(tf.expand_dims(predictions[i], axis=0)), dtype=np.float64)[0][0]\n",
    "        }\n",
    "        # we write dataset for each txt file\n",
    "        with open(directory_to_write + filename_to_write + \"_prediction_\" + str(i) + \".json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(dictionary_to_write, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_vae_optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mo_vae_train(train_dataset, validation_dataset, preview_geometry, epochs):\n",
    "    \n",
    "    noise_vector = tf.random.normal([num_examples_to_generate, latent_dimension], seed=101)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_start = time.time()\n",
    "        \n",
    "        train_loss_vae = 0.0\n",
    "        train_obj_1_losses = 0\n",
    "        train_obj_2_losses = 0\n",
    "        train_obj_3_losses = 0\n",
    "        train_iters = 0\n",
    "        \n",
    "        for each_batch in train_dataset:\n",
    "            total_loss_train, train_obj_1_loss, train_obj_2_loss, train_obj_3_loss = mo_vae_train_step(mo_vae_optimizer, each_batch, each_batch, train=True)\n",
    "            train_obj_1_losses += train_obj_1_loss\n",
    "            train_obj_2_losses += train_obj_2_loss\n",
    "            train_obj_3_losses += train_obj_3_loss\n",
    "            train_loss_vae += total_loss_train\n",
    "            train_iters += 1\n",
    "            if train_iters == int(num_training_samples/batch_size):\n",
    "                break\n",
    "        \n",
    "        validation_loss_vae = 0.0\n",
    "        val_obj_1_losses = 0\n",
    "        val_obj_2_losses = 0\n",
    "        val_obj_3_losses = 0\n",
    "        validation_iters = 0\n",
    "\n",
    "        for each_batch in validation_dataset:\n",
    "            total_loss_val, val_obj_1_loss, val_obj_2_loss, val_obj_3_loss = mo_vae_train_step(mo_vae_optimizer, each_batch, each_batch, train=False)\n",
    "            val_obj_1_losses += val_obj_1_loss\n",
    "            val_obj_2_losses += val_obj_2_loss\n",
    "            val_obj_3_losses += val_obj_3_loss\n",
    "            validation_loss_vae += total_loss_val\n",
    "            validation_iters += 1\n",
    "            if validation_iters == int(num_validation_samples/batch_size):\n",
    "                break\n",
    "\n",
    "        log_file = open(mo_vae_training_log_location, \"a\")\n",
    "        log_file.write('\\n')\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Epoch: \" + str(epoch))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Training loss: {:0.5f}\".format(train_loss_vae/train_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Objective stability training loss: {:0.5f}\".format(train_obj_1_losses/train_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Objective aesthetic training loss: {:0.5f}\".format(train_obj_2_losses/train_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Objective function training loss: {:0.5f}\".format(train_obj_3_losses/train_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Validation loss: {:0.5f}\".format(validation_loss_vae/validation_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Objective stability validation loss: {:0.5f}\".format(val_obj_1_losses/validation_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Objective aesthetic validation loss: {:0.5f}\".format(val_obj_2_losses/validation_iters))\n",
    "        log_file.write('\\n')\n",
    "        log_file.write(\"Objective function validation loss: {:0.5f}\".format(val_obj_3_losses/validation_iters))\n",
    "        log_file.close()\n",
    "\n",
    "        write_sample_json(conv3d_vae, noise_vector, directory_to_generated_samples, \"epoch_\" + str(epoch))\n",
    "        \n",
    "        display.clear_output(wait=False)\n",
    "        \n",
    "        print(\"Original input: \")\n",
    "        preview_geometries(preview_geometry, stability_model=stability_auxiliary_discriminator_model, aesthetic_model=aesthetic_auxiliary_discriminator_model, function_model=function_auxiliary_discriminator_model)\n",
    "        print(\"Generated output: \")\n",
    "        generated_geometry = np.squeeze(conv3d_vae.sample_from_input(np.expand_dims(preview_geometry, axis=0)), axis=0)\n",
    "        preview_geometries(generated_geometry, plot_threshold=0.25, stability_model=stability_auxiliary_discriminator_model, aesthetic_model=aesthetic_auxiliary_discriminator_model, function_model=function_auxiliary_discriminator_model)\n",
    "        \n",
    "        print('Epoch: {}, Training Loss: {:0.5f}, Validation Loss: {:0.5f}, Time: {:0.1f}'.format(epoch, train_loss_vae/train_iters, validation_loss_vae/validation_iters, time.time() - train_start))\n",
    "\n",
    "        # For every 10 epochs, we save the network weights\n",
    "        if epoch % 10 == 0:\n",
    "            conv3d_vae.save_weights(mo_vae_save_directory + 'mo_vae_epoch_{}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for each_batch in val_ds:\n",
    "    preview_geometry = each_batch[0]\n",
    "    break\n",
    "mo_vae_train(train_ds, val_ds, preview_geometry, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eV3pHWbReqi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jna2p9QAUWM6",
    "pTegEa4k1RXi"
   ],
   "machine_shape": "hm",
   "name": "210611 - MO_VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
